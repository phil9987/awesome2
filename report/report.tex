\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Introduction to Learning and Intelligent Systems -- Spring 2015}
\author{taubnert@student.ethz.ch\\ junkerp@student.ethz.ch\\ kellersu@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section{Project Regression -- Team ``awesome''}

%Briefly describe the steps used to produce the solution. Feel
%free to add plots or screenshots if you think it's necessary. The
%report should contain a maximum of 2 pages.

\subsection{Classifiers}
We used a number of different classifiers, like k-nn, svm, randomforest and extratree.

In the end, we achieved the best results with the extratree-classifier, using the features as they are provided, without even scaling them.
We used a grid-search to find the best parameters for ``max\_features'', ``min\_samples\_split'' and ``max\_depth''.

The following parameters for the ExtraTree Classifier turned out to perform best:
criterion=``entropy''
max\_features=108
min\_samples\_split=4
n\_estimators=421


We found out, that the max\_features and the n\_estimators need to be high where as max\_depth should be rather low, to avoid overfitting of the classifier.


\subsection{Features}
We used all the provided features without changing them and added for the first 9 features also their polynomials up to degree 3.
This lead to a much higher amout of features (273 instead of 53) and to a much longer evaluation time but unfortunately not a big improvement of the predictions.

\end{document}
